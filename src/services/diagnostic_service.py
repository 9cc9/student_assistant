"""å…¥å­¦è¯Šæ–­æœåŠ¡ - ç”Ÿæˆå­¦ç”Ÿç”»åƒçš„æ ¸å¿ƒæœåŠ¡"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import json
import os

from ..models.student import StudentProfile, LearningLevel, LearningStyle
from .ai_scoring_service import get_ai_scoring_service

logger = logging.getLogger(__name__)


class DiagnosticService:
    """
    å…¥å­¦è¯Šæ–­æœåŠ¡
    
    é€šè¿‡10-15åˆ†é’Ÿçš„åŸºçº¿æµ‹è¯„ï¼ŒåŒ…æ‹¬ï¼š
    1. æ¦‚å¿µå°æµ‹ - è¯„ä¼°åŸºç¡€æ¦‚å¿µç†è§£
    2. ä»£ç å°é¢˜ - è¯„ä¼°ç¼–ç¨‹èƒ½åŠ›
    3. å·¥å…·ç†Ÿæ‚‰åº¦é—®å· - è¯„ä¼°æŠ€èƒ½ç†Ÿæ‚‰ç¨‹åº¦
    4. å­¦ä¹ åå¥½è°ƒæŸ¥ - äº†è§£å­¦ä¹ é£æ ¼å’Œæ—¶é—´å®‰æ’
    
    ç”Ÿæˆä¸ªæ€§åŒ–å­¦ç”Ÿç”»åƒç”¨äºåç»­è·¯å¾„æ¨èã€‚
    """
    
    def __init__(self):
        self.diagnostic_data = self._load_diagnostic_questions()
        self.ai_scoring = get_ai_scoring_service()  # åˆå§‹åŒ–AIè¯„åˆ†æœåŠ¡
        logger.info(f"ğŸ§ª è¯Šæ–­æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ŒAIè¯„åˆ†: {'å·²å¯ç”¨' if self.ai_scoring.is_enabled() else 'æœªå¯ç”¨'}")
    
    def _load_diagnostic_questions(self) -> Dict[str, Any]:
        """ä»JSONæ–‡ä»¶åŠ è½½è¯Šæ–­é¢˜ç›®"""
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            json_path = os.path.join(project_root, 'config', 'diagnostic_questions.json')
            
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"âœ… æˆåŠŸä»JSONæ–‡ä»¶åŠ è½½è¯Šæ–­é¢˜ç›®: {json_path}")
                return data
        except FileNotFoundError:
            logger.error(f"âŒ è¯Šæ–­é¢˜ç›®JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            raise DiagnosticServiceError("è¯Šæ–­é¢˜ç›®æ–‡ä»¶ä¸å­˜åœ¨")
        except json.JSONDecodeError as e:
            logger.error(f"âŒ è¯Šæ–­é¢˜ç›®JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise DiagnosticServiceError("è¯Šæ–­é¢˜ç›®æ–‡ä»¶æ ¼å¼é”™è¯¯")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è¯Šæ–­é¢˜ç›®å¤±è´¥: {e}")
            raise DiagnosticServiceError(f"åŠ è½½è¯Šæ–­é¢˜ç›®å¤±è´¥: {e}")
    
    
    def get_diagnostic_test(self) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´çš„å…¥å­¦è¯Šæ–­æµ‹è¯•
        
        è¿”å›åŒ…å«æ‰€æœ‰æµ‹è¯•é¢˜ç›®å’Œè°ƒæŸ¥é—®å·çš„ç»“æ„åŒ–æ•°æ®
        """
        return self.diagnostic_data
    
    def evaluate_diagnostic_results(self, student_responses: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°è¯Šæ–­æµ‹è¯•ç»“æœï¼Œç”Ÿæˆå­¦ç”Ÿç”»åƒæ•°æ®
        
        Args:
            student_responses: å­¦ç”Ÿçš„å›ç­”æ•°æ®
            
        Returns:
            åŒ…å«å„ç»´åº¦å¾—åˆ†å’Œç”»åƒä¿¡æ¯çš„ç»“æœ
        """
        try:
            logger.info(f"ğŸ§ª å¼€å§‹è¯„ä¼°è¯Šæ–­ç»“æœ")
            
            # è¯„ä¼°æ¦‚å¿µç†è§£
            concept_score = self._evaluate_concepts(student_responses.get("concepts", {}))
            
            # è¯„ä¼°ç¼–ç¨‹èƒ½åŠ›
            coding_score = self._evaluate_coding(student_responses.get("coding", {}))
            
            # è¯„ä¼°å·¥å…·ç†Ÿæ‚‰åº¦
            tool_familiarity_score, skill_scores = self._evaluate_tools(student_responses.get("tools", {}))
            
            # åˆ†æå­¦ä¹ åå¥½
            preferences = self._analyze_preferences(student_responses.get("preferences", {}))
            
            # ç”Ÿæˆç»¼åˆè¯„ä¼°ç»“æœ
            results = {
                "concept_score": concept_score,
                "coding_score": coding_score,  
                "tool_familiarity": tool_familiarity_score,
                "skill_scores": skill_scores,
                "learning_style_preference": preferences["learning_style"],
                "time_budget_hours_per_week": preferences["time_budget"],
                "interests": preferences["interests"],
                "goals": preferences["goals"],
                "challenges": preferences["challenges"],
                "overall_readiness": self._calculate_overall_readiness(
                    concept_score, coding_score, tool_familiarity_score
                ),
                "recommendations": self._generate_initial_recommendations(
                    concept_score, coding_score, tool_familiarity_score, preferences
                ),
                "evaluated_at": datetime.now().isoformat()
            }
            
            logger.info(f"ğŸ§ª âœ… è¯Šæ–­è¯„ä¼°å®Œæˆï¼Œæ•´ä½“å‡†å¤‡åº¦: {results['overall_readiness']}")
            return results
            
        except Exception as e:
            logger.error(f"ğŸ§ª âŒ è¯Šæ–­è¯„ä¼°å¤±è´¥: {str(e)}")
            raise DiagnosticServiceError(f"è¯Šæ–­è¯„ä¼°å¤±è´¥: {str(e)}")
    
    def _evaluate_concepts(self, concept_responses: Dict[str, Any]) -> int:
        """è¯„ä¼°æ¦‚å¿µç†è§£å¾—åˆ†"""
        total_score = 0
        max_score = 0
        
        # ä»JSONæ•°æ®ä¸­è·å–æ¦‚å¿µæµ‹è¯•é¢˜ç›®
        concepts_section = next((s for s in self.diagnostic_data["sections"] if s["id"] == "concepts"), None)
        if not concepts_section:
            logger.warning("æœªæ‰¾åˆ°æ¦‚å¿µæµ‹è¯•é¢˜ç›®")
            return 0
            
        for question in concepts_section["questions"]:
            max_score += question["weight"]
            student_answer = concept_responses.get(question["id"])
            
            if question["type"] == "multiple_choice":
                if student_answer == question["correct_answer"]:
                    total_score += question["weight"]
                    logger.info(f"  âœ… {question['id']}: é€‰æ‹©é¢˜ç­”å¯¹ +{question['weight']}åˆ†")
                else:
                    logger.info(f"  âŒ {question['id']}: é€‰æ‹©é¢˜ç­”é”™")
            elif question["type"] == "short_answer":
                # AIæ™ºèƒ½è¯„åˆ†æˆ–å…³é”®è¯åŒ¹é…è¯„åˆ†
                if student_answer:
                    score = self._score_short_answer(
                        student_answer, 
                        question["sample_answer"],
                        question_text=question.get("question", "")
                    )
                    earned = int(question["weight"] * score)
                    total_score += earned
                    logger.info(f"  {'âœ…' if score > 0.6 else 'âš ï¸'} {question['id']}: ç®€ç­”é¢˜ +{earned}/{question['weight']}åˆ†")
                else:
                    logger.info(f"  âš ï¸ {question['id']}: æœªä½œç­”")
        
        return int((total_score / max_score) * 100) if max_score > 0 else 0
    
    def _evaluate_coding(self, coding_responses: Dict[str, Any]) -> int:
        """è¯„ä¼°ç¼–ç¨‹èƒ½åŠ›å¾—åˆ†"""
        total_score = 0
        max_score = 0
        
        # ä»JSONæ•°æ®ä¸­è·å–ç¼–ç¨‹æµ‹è¯•é¢˜ç›®
        coding_section = next((s for s in self.diagnostic_data["sections"] if s["id"] == "coding"), None)
        if not coding_section:
            logger.warning("æœªæ‰¾åˆ°ç¼–ç¨‹æµ‹è¯•é¢˜ç›®")
            return 0
            
        for question in coding_section["questions"]:
            max_score += question["weight"]
            student_answer = coding_responses.get(question["id"])
            
            if question["type"] == "coding":
                if student_answer:
                    # AIæ™ºèƒ½è¯„åˆ†æˆ–ç®€åŒ–ä»£ç è¯„ä¼°
                    score = self._score_coding_answer(student_answer, question)
                    earned = int(question["weight"] * score)
                    total_score += earned
                    logger.info(f"  {'âœ…' if score > 0.6 else 'âš ï¸'} {question['id']}: ç¼–ç¨‹é¢˜ +{earned}/{question['weight']}åˆ†")
                else:
                    logger.info(f"  âš ï¸ {question['id']}: æœªæäº¤ä»£ç ")
            elif question["type"] == "code_analysis":
                if student_answer:
                    score = self._score_analysis_answer(student_answer, question)
                    earned = int(question["weight"] * score)
                    total_score += earned
                    logger.info(f"  {'âœ…' if score > 0.6 else 'âš ï¸'} {question['id']}: ä»£ç åˆ†æé¢˜ +{earned}/{question['weight']}åˆ†")
                else:
                    logger.info(f"  âš ï¸ {question['id']}: æœªä½œç­”")
        
        return int((total_score / max_score) * 100) if max_score > 0 else 0
    
    def _evaluate_tools(self, tool_responses: Dict[str, Any]) -> tuple[int, Dict[str, int]]:
        """è¯„ä¼°å·¥å…·ç†Ÿæ‚‰åº¦"""
        skill_scores = {}
        category_scores = []
        
        # ä»JSONæ•°æ®ä¸­è·å–å·¥å…·è°ƒæŸ¥
        tools_section = next((s for s in self.diagnostic_data["sections"] if s["id"] == "tools"), None)
        if not tools_section:
            logger.warning("æœªæ‰¾åˆ°å·¥å…·è°ƒæŸ¥æ•°æ®")
            return 0, {}
            
        for category in tools_section["survey"]:
            category_name = category["category"]
            category_total = 0
            tool_count = len(category["tools"])
            
            for tool in category["tools"]:
                tool_name = tool["name"]
                # å­¦ç”Ÿå¯¹å·¥å…·çš„ç†Ÿæ‚‰åº¦è¯„åˆ†ï¼ˆ1-5ï¼‰
                familiarity = tool_responses.get(tool_name, 1)
                skill_scores[tool_name] = familiarity * 20  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
                category_total += familiarity
            
            # è®¡ç®—ç±»åˆ«å¹³å‡åˆ†
            if tool_count > 0:
                category_avg = (category_total / tool_count) * 20
                category_scores.append(category_avg)
        
        # è®¡ç®—æ€»ä½“å·¥å…·ç†Ÿæ‚‰åº¦
        overall_score = int(sum(category_scores) / len(category_scores)) if category_scores else 0
        
        return overall_score, skill_scores
    
    def _analyze_preferences(self, preference_responses: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ åå¥½"""
        return {
            "learning_style": preference_responses.get("learning_style", "examples_first"),
            "time_budget": preference_responses.get("time_budget", 6),
            "interests": preference_responses.get("interests", []),
            "goals": preference_responses.get("goals", []),
            "challenges": preference_responses.get("challenges", "concepts")
        }
    
    def _score_short_answer(self, student_answer: str, sample_answer: str, question_text: str = "") -> float:
        """
        ç®€ç­”é¢˜è¯„åˆ†ï¼ˆä¼˜å…ˆä½¿ç”¨AIè¯„åˆ†ï¼‰
        
        Args:
            student_answer: å­¦ç”Ÿç­”æ¡ˆ
            sample_answer: å‚è€ƒç­”æ¡ˆ
            question_text: é¢˜ç›®å†…å®¹ï¼ˆç”¨äºAIè¯„åˆ†ï¼‰
            
        Returns:
            0.0-1.0ä¹‹é—´çš„å¾—åˆ†ç‡
        """
        if not student_answer:
            return 0.0
        
        # å°è¯•ä½¿ç”¨AIè¯„åˆ†
        if self.ai_scoring.is_enabled() and question_text:
            try:
                result = self.ai_scoring.score_short_answer(
                    question=question_text,
                    student_answer=student_answer,
                    reference_answer=sample_answer,
                    max_score=100
                )
                score_rate = result['score'] / 100.0
                logger.info(f"  ğŸ“ AIè¯„åˆ†: {result['score']}/100 - {result['feedback']}")
                return score_rate
            except Exception as e:
                logger.warning(f"  âš ï¸ AIè¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†: {str(e)}")
        
        # å¤‡ç”¨ï¼šç®€å•çš„å…³é”®è¯åŒ¹é…
        student_words = set(student_answer.lower().split())
        sample_words = set(sample_answer.lower().split())
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        common_words = student_words.intersection(sample_words)
        if len(sample_words) == 0:
            return 0.0
        
        similarity = len(common_words) / len(sample_words)
        score_rate = min(similarity * 1.2, 1.0)
        logger.info(f"  ğŸ“ è§„åˆ™è¯„åˆ†: {int(score_rate * 100)}/100")
        return score_rate
    
    def _score_coding_answer(self, student_code: str, question: Dict[str, Any]) -> float:
        """
        ç¼–ç¨‹é¢˜è¯„åˆ†ï¼ˆä¼˜å…ˆä½¿ç”¨AIè¯„åˆ†ï¼‰
        
        Args:
            student_code: å­¦ç”Ÿä»£ç 
            question: é¢˜ç›®ä¿¡æ¯
            
        Returns:
            0.0-1.0ä¹‹é—´çš„å¾—åˆ†ç‡
        """
        if not student_code:
            return 0.0
        
        # å°è¯•ä½¿ç”¨AIè¯„åˆ†
        if self.ai_scoring.is_enabled():
            try:
                requirements = question.get("evaluation_criteria", [
                    "ä»£ç åŠŸèƒ½æ­£ç¡®",
                    "ä»£ç ç»“æ„æ¸…æ™°", 
                    "åŒ…å«å¿…è¦çš„é”™è¯¯å¤„ç†"
                ])
                
                result = self.ai_scoring.score_coding_question(
                    question=question.get("question", ""),
                    student_code=student_code,
                    requirements=requirements,
                    max_score=100
                )
                score_rate = result['score'] / 100.0
                logger.info(f"  ğŸ’» AIè¯„åˆ†: {result['score']}/100 - {result['feedback']}")
                return score_rate
            except Exception as e:
                logger.warning(f"  âš ï¸ AIè¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†: {str(e)}")
        
        # å¤‡ç”¨ï¼šåŸºç¡€ç»“æ„æ£€æŸ¥
        score = 0.0
        
        # åŸºç¡€ç»“æ„æ£€æŸ¥
        if "def " in student_code:
            score += 0.3
        if "return " in student_code:
            score += 0.3
        
        # å…³é”®é€»è¾‘æ£€æŸ¥ï¼ˆæ ¹æ®é¢˜ç›®ç±»å‹ï¼‰
        if question["id"] == "code_1":  # æŸ¥æ‰¾æœ€å¤§å€¼
            if "max(" in student_code or ">" in student_code:
                score += 0.4
        elif question["id"] == "code_2":  # APIè°ƒç”¨å’Œé”™è¯¯å¤„ç†
            if "try:" in student_code or "except" in student_code:
                score += 0.2
            if "requests." in student_code:
                score += 0.2
        
        logger.info(f"  ğŸ’» è§„åˆ™è¯„åˆ†: {int(score * 100)}/100")
        return min(score, 1.0)
    
    def _score_analysis_answer(self, student_answer: str, question: Dict[str, Any]) -> float:
        """
        ä»£ç åˆ†æé¢˜è¯„åˆ†ï¼ˆä¼˜å…ˆä½¿ç”¨AIè¯„åˆ†ï¼‰
        
        Args:
            student_answer: å­¦ç”Ÿçš„åˆ†æ
            question: é¢˜ç›®ä¿¡æ¯
            
        Returns:
            0.0-1.0ä¹‹é—´çš„å¾—åˆ†ç‡
        """
        if not student_answer:
            return 0.0
        
        # å°è¯•ä½¿ç”¨AIè¯„åˆ†
        if self.ai_scoring.is_enabled():
            try:
                result = self.ai_scoring.score_code_analysis(
                    question=question.get("question", ""),
                    code_snippet=question.get("code", ""),
                    student_analysis=student_answer,
                    max_score=100
                )
                score_rate = result['score'] / 100.0
                logger.info(f"  ğŸ” AIè¯„åˆ†: {result['score']}/100 - {result['feedback']}")
                return score_rate
            except Exception as e:
                logger.warning(f"  âš ï¸ AIè¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†: {str(e)}")
        
        # å¤‡ç”¨ï¼šç®€åŒ–è¯„åˆ†ï¼Œæ£€æŸ¥å…³é”®æ¦‚å¿µ
        key_concepts = ["å¼•ç”¨", "åˆ—è¡¨", "append", "ä¿®æ”¹", "åŒä¸€ä¸ªå¯¹è±¡"]
        score = 0.0
        
        for concept in key_concepts:
            if concept in student_answer:
                score += 0.2
        
        logger.info(f"  ğŸ” è§„åˆ™è¯„åˆ†: {int(score * 100)}/100")
        return min(score, 1.0)
    
    def _calculate_overall_readiness(self, concept_score: int, coding_score: int, tool_score: int) -> str:
        """è®¡ç®—æ•´ä½“å‡†å¤‡åº¦ç­‰çº§"""
        avg_score = (concept_score + coding_score + tool_score) / 3
        
        if avg_score >= 85:
            return "ä¼˜ç§€"
        elif avg_score >= 70:
            return "è‰¯å¥½"
        elif avg_score >= 50:
            return "åˆæ ¼"
        else:
            return "éœ€è¦åŠ å¼º"
    
    def _generate_initial_recommendations(
        self, 
        concept_score: int, 
        coding_score: int, 
        tool_score: int,
        preferences: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆåˆå§‹å»ºè®®"""
        recommendations = []
        
        # åŸºäºå¾—åˆ†çš„å»ºè®®
        if concept_score < 60:
            recommendations.append("å»ºè®®å…ˆå¤ä¹ åŸºç¡€æŠ€æœ¯æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯APIã€HTTPç­‰ç½‘ç»œåŸºç¡€çŸ¥è¯†")
        
        if coding_score < 60:
            recommendations.append("å»ºè®®åŠ å¼ºç¼–ç¨‹åŸºç¡€ç»ƒä¹ ï¼Œé‡ç‚¹å…³æ³¨Pythonè¯­æ³•å’Œé”™è¯¯å¤„ç†")
        
        if tool_score < 60:
            recommendations.append("å»ºè®®å…ˆç†Ÿæ‚‰å¼€å‘ç¯å¢ƒå’ŒåŸºç¡€å·¥å…·ï¼Œå¦‚Gitã€å‘½ä»¤è¡Œç­‰")
        
        # åŸºäºå­¦ä¹ æŒ‘æˆ˜çš„å»ºè®®
        challenge = preferences.get("challenges", "")
        if challenge == "debugging":
            recommendations.append("æ¨èä½¿ç”¨IDEè°ƒè¯•åŠŸèƒ½ï¼Œå¤šç»ƒä¹ é”™è¯¯å®šä½æŠ€å·§")
        elif challenge == "time_management":
            recommendations.append("å»ºè®®åˆ¶å®šè¯¦ç»†çš„å­¦ä¹ è®¡åˆ’ï¼Œè®¾ç½®é˜¶æ®µæ€§ç›®æ ‡")
        elif challenge == "motivation":
            recommendations.append("å»ºè®®é€‰æ‹©ä¸ä¸ªäººå…´è¶£ç›¸å…³çš„é¡¹ç›®è¿›è¡Œå®è·µ")
        
        # åŸºäºå…´è¶£çš„å»ºè®®
        interests = preferences.get("interests", [])
        if "RAG" in interests:
            recommendations.append("å¯ä»¥é‡ç‚¹å…³æ³¨RAGç³»ç»Ÿæ„å»ºï¼Œè¿™ä¸ä½ çš„å…´è¶£åŒ¹é…")
        if "ç§»åŠ¨ç«¯" in interests:
            recommendations.append("å»ºè®®åœ¨UIè®¾è®¡å’Œå‰ç«¯å¼€å‘ç¯èŠ‚æŠ•å…¥æ›´å¤šç²¾åŠ›")
        
        if not recommendations:
            recommendations.append("ä½ çš„åŸºç¡€ä¸é”™ï¼Œå¯ä»¥æŒ‰æ ‡å‡†è·¯å¾„å­¦ä¹ ï¼Œé€‚å½“æŒ‘æˆ˜é«˜éš¾åº¦ä»»åŠ¡")
        
        return recommendations


class DiagnosticServiceError(Exception):
    """è¯Šæ–­æœåŠ¡é”™è¯¯"""
    pass
